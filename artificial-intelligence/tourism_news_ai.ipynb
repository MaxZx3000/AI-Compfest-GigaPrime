{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMCd07L7EdZu"
      },
      "source": [
        "# Tourism News AI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nlp_id\n",
        "! pip install beautifulsoup4\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxFYOvs9V-b6",
        "outputId": "66b8075e-ef84-40bf-fce9-1702f3d3ba89"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nlp_id in /usr/local/lib/python3.7/dist-packages (0.1.12.0)\n",
            "Requirement already satisfied: wget==3.2 in /usr/local/lib/python3.7/dist-packages (from nlp_id) (3.2)\n",
            "Requirement already satisfied: scikit-learn==0.22 in /usr/local/lib/python3.7/dist-packages (from nlp_id) (0.22)\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from nlp_id) (3.4.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->nlp_id) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22->nlp_id) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22->nlp_id) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22->nlp_id) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPM0be2GEvQ2"
      },
      "source": [
        "Library Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "d4-97qDJDzmf",
        "outputId": "e80e055b-ab39-4c01-8ddb-821748a5bb32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://www.googleapis.com/customsearch/v1?key=AIzaSyAsQSjiOTOUOqz-oRrp71bdmHmoa9eLqyc&cx=b513cee2778d74c33&q=Monumen+Nasional+Berita'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import nltk\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from nlp_id.lemmatizer import Lemmatizer \n",
        "from nlp_id.tokenizer import Tokenizer\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "google_url = \"https://www.googleapis.com/customsearch/v1?key=AIzaSyAsQSjiOTOUOqz-oRrp71bdmHmoa9eLqyc&cx=b513cee2778d74c33&q=Monumen+Nasional+Berita\"\n",
        "google_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYOm5wXGEzP7"
      },
      "source": [
        "Rqeuest Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "ClC6wKmfE0es",
        "outputId": "20254935-17c0-47ea-e9bc-c41890bcd3e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"kind\": \"customsearch#search\",\\n  \"url\": {\\n    \"type\": \"application/json\",\\n    \"template\": \"https://www.googleapis.com/customsearch/v1?q={searchTerms}&num={count?}&start={startIndex?}&lr={language?}&safe={safe?}&cx={cx?}&sort={sort?}&filter={filter?}&gl={gl?}&cr={cr?}&googlehost={googleHost?}&c2coff={disableCnTwTranslation?}&hq={hq?}&hl={hl?}&siteSearch={siteSearch?}&siteSearchFilter={siteSearchFilter?}&exactTerms={exactTerms?}&excludeTerms={excludeTerms?}&linkSite={linkSite?}&orTerms={orTerms?}&relatedSite={relatedSite?}&dateRestrict={dateRestrict?}&lowRange={lowRange?}&highRange={highRange?}&searchType={searchType}&fileType={fileType?}&rights={rights?}&imgSize={imgSize?}&imgType={imgType?}&imgColorType={imgColorType?}&imgDominantColor={imgDominantColor?}&alt=json\"\\n  },\\n  \"queries\": {\\n    \"request\": [\\n      {\\n        \"title\": \"Google Custom Search - Monumen Nasional Berita\",\\n        \"totalResults\": \"352000\",\\n        \"searchTerms\": \"Monumen Nasional Berita\",\\n        \"count\": 10,\\n   '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "r = requests.get(google_url)\n",
        "r.text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_response = json.loads(r.text)"
      ],
      "metadata": {
        "id": "SSwRJkdq4Pp-"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get All URLS"
      ],
      "metadata": {
        "id": "nid_8HhZzTeV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXtA8za-FSBL",
        "outputId": "43afb2a4-38a0-46c6-a934-0e30ac5b9ca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.kompas.com/stori/read/2022/04/18/170000379/sejarah-monumen-nasional-monas-?page=all\n",
            "https://www.tribunnews.com/nasional/2020/11/17/upt-monas-tolak-permohonan-izin-penggunaan-kawasan-monumen-nasional-untuk-reuni-212?page=all\n",
            "https://www.tribunnews.com/regional/2012/01/06/monumen-nasional-keadilan-dibangun-di-medan?page=all\n",
            "https://www.tribunnews.com/metropolitan/2014/05/12/si-kembar-semringah-bisa-ikut-cuci-monumen-nasional?page=all\n",
            "https://www.kompas.com/skola/read/2020/01/05/120000069/sejarah-pembangunan-monas?page=all\n",
            "https://www.tribunnews.com/metropolitan/2014/05/12/nasionalisme-ahok-dan-pencucian-monumen-nasional?page=all\n",
            "https://www.cnnindonesia.com/nasional/20211231145022-20-741079/jelang-malam-tahun-baru-kawasan-monas-ditutup?page=all\n",
            "https://www.liputan6.com/news/read/4987818/monas-akan-dibuka-untuk-umum-pekan-ini?page=all\n",
            "https://www.tribunnews.com/metropolitan/2022/06/19/aturan-masuk-monas-harga-tiket-masuk-dan-syarat-menggunakan-lapangan-monas?page=all\n",
            "https://www.cnnindonesia.com/nasional/20220615193416-20-809503/wagub-dki-monas-akan-kembali-dibuka-pekan-ini?page=all\n"
          ]
        }
      ],
      "source": [
        "link_urls = []\n",
        "\n",
        "for item in json_response[\"items\"]:\n",
        "  link = item['link']\n",
        "  if link.endswith(\"?page=all\") == False:\n",
        "    link += \"?page=all\"\n",
        "\n",
        "  link_urls.append(link)\n",
        "  print(link)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the Content using BeautifulSoup"
      ],
      "metadata": {
        "id": "dQpbIkZw5S_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_content_from_scraper_api(url):\n",
        "  API_KEY = \"aa2c2fb68fc1cf60e13db8ebaf260112\"\n",
        "  web_url = f\"http://api.scraperapi.com?api_key={API_KEY}&url={url}\"\n",
        "  request = requests.get(web_url)\n",
        "  return request.text\n",
        "\n",
        "def get_relevant_paragraphs_only(web_content):\n",
        "  beautiful_soup = BeautifulSoup(web_content)\n",
        "  texts = [element.text for element in beautiful_soup.find_all('p', {'class': None, 'id': None}) ]\n",
        "  # texts = beautiful_soup.find_all('p', {'class': None, 'id': None})\n",
        "  return texts\n",
        "\n",
        "all_relevant_paragraphs = []\n",
        "\n",
        "for url in link_urls:\n",
        "  web_content = retrieve_content_from_scraper_api(url)\n",
        "  relevant_paragraphs = get_relevant_paragraphs_only(web_content)\n",
        "  all_relevant_paragraphs.append(relevant_paragraphs)\n",
        "  break\n",
        "  # print(relevant_paragraphs)"
      ],
      "metadata": {
        "id": "JC8ZhPM_z31M"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_relevant_paragraphs[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGDqYkmS6eVC",
        "outputId": "72d4e458-9c7d-4ff5-8704-a031b63b2d5a"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['KOMPAS.com -\\xa0Monumen Nasional atau Monas adalah tugu yang dibangun dengan tujuan mengenang sejarah perjuangan di Jakarta.',\n",
              " 'Monas terletak di pusat Kota Jakarta, yang saat ini dijadikan sebagai tempat wisata dan pusat pendidikan bagi para pengunjung dari Indonesia maupun wisatawan asing.',\n",
              " 'Apabila menilik sejarahnya, Monas mulai dibangun pada 17 Agustus 1961. Arsitek Monumen Nasional adalah Soedarsono, Frederich Silaban, dan Ir. Rooseno.',\n",
              " 'Monas kemudian diresmikan dan dibuka untuk umum pada 1975. Berikut ini sejarah Monas.',\n",
              " 'Baca juga: Sejarah Pembangunan Monas',\n",
              " 'Gagasan untuk mendirikan Monas sudah ada sejak 1954. Beberapa hari setelah peringatan proklamasi kemerdekaan Indonesia ke-9, dibentuk Panitia Tugu Nasional yang bertugas untuk mengupayakan berdirinya Tugu Monas.',\n",
              " 'Panitia ini diketuai oleh Sarwoko Martokusumo, dengan dibantu oleh S Suhud sebagai penulis, Sumali Prawirosudirdjo sebagai bendahara, dan empat anggota lainnya, yaitu Supeno, KK Wiloto, EF Wenas, dan Sudiro.',\n",
              " 'Panitia Tugu Nasional bertanggung jawab untuk mempersiapkan segala hal yang dibutuhkan guna membangun Tugu Monas sekaligus mengumpulkan biaya pembangunannya.',\n",
              " 'Setelah itu, Presiden Soekarno membentuk panitia pembangunan Monas bernama Tim Yuri.',\n",
              " 'Tim ini melakukan dua kali pertemuan, yaitu 17 pada Februari 1955 dan 10 Mei 1960, untuk merancang bentuk bangunan Tugu Monas.']"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "S0ZAEXn7V3Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indo_tokenizer = Tokenizer()\n",
        "indo_lemmatizer = Lemmatizer()\n",
        "\n",
        "def perform_lemmatizer(word):\n",
        "    return indo_lemmatizer.lemmatize(word)"
      ],
      "metadata": {
        "id": "LSSTUNkhV4yN"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create TextRank to Summarize Text"
      ],
      "metadata": {
        "id": "aUHAo1T0VEUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(sentences, word_length_threshold = 5):\n",
        "  sentence_tokenized = sent_tokenize(sentences)\n",
        "  sentence_filtered_tokenized = [sentence for sentence in sentence_tokenized if len(sentence) >= word_length_threshold]\n",
        "  return sentence_filtered_tokenized"
      ],
      "metadata": {
        "id": "ZjNiuisxUZVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextRank():\n",
        "  def _get_text_rank_matrix(self, sentence_tokenized, alpha = 0.3):\n",
        "    tf_idf_vectorizer = TfidfVectorizer(norm = 'l1')\n",
        "    \n",
        "    tf_idf_matrix = tf_idf_vectorizer.fit_transform(sentence_tokenized)\n",
        "\n",
        "    cosine_similarity_matrix = cosine_similarity(\n",
        "        tf_idf_matrix\n",
        "    )\n",
        "\n",
        "    sum_all_rows = cosine_similarity_matrix.sum(axis = 1, dtype = 'float64')\n",
        "    normalized_cosine_similarity_matrix = cosine_similarity_matrix / sum_all_rows.reshape(-1, 1)\n",
        "    number_of_sentences = normalized_cosine_similarity_matrix.shape[0]\n",
        "    uniform_matrix = np.full(fill_value = 1 / number_of_sentences, shape = (number_of_sentences, number_of_sentences))\n",
        "    text_rank_matrix = alpha * uniform_matrix + (1 - alpha) * normalized_cosine_similarity_matrix\n",
        "\n",
        "    # For debugging purpose.\n",
        "    if np.isnan(text_rank_matrix).any():\n",
        "      # print(type(text_rank_matrix.todense()))\n",
        "      for sentence in sentence_tokenized:\n",
        "        print(f\"'{sentence}'\")\n",
        "      for vector in text_rank_matrix:\n",
        "        print(vector)\n",
        "\n",
        "    eigenvalues, normalized_eigenvectors = np.linalg.eig(text_rank_matrix.T)\n",
        "\n",
        "    return text_rank_matrix\n",
        "\n",
        "  def _get_scores(self, text_rank_matrix):\n",
        "    eigenvalues, normalized_eigenvectors = np.linalg.eig(text_rank_matrix.T)\n",
        "    valid_index = self._get_index_from_specified_value(eigenvalues, 1.)[0]\n",
        "    scores = normalized_eigenvectors[:, valid_index]\n",
        "    scores = normalized_eigenvectors[:, valid_index] / normalized_eigenvectors[:, valid_index].sum()\n",
        "    return scores\n",
        "\n",
        "  def _get_summarized_text(self, sentences, scores, max_largest_index):\n",
        "    if len(scores) - 1 < max_largest_index:\n",
        "      return sentences\n",
        "    \n",
        "    summarized_texts = []\n",
        "    threshold = sorted(scores, reverse = True)[max_largest_index - 1]\n",
        "\n",
        "    for sentence, score in zip(sentences, scores):\n",
        "      if score >= threshold:\n",
        "        summarized_texts.append(sentence)\n",
        "\n",
        "    return summarized_texts\n",
        "\n",
        "  def summarize_text(self, sentences, index, max_largest_index = 5, alpha = 0.15, word_length_threshold = 10):\n",
        "    # Remove any words with less than given length of sentences.\n",
        "    sentence_tokenized = preprocess_text(sentences, word_length_threshold)\n",
        "  \n",
        "    text_rank_matrix = self._get_page_rank_matrix(sentence_tokenized, alpha)\n",
        "    scores = self._get_scores(text_rank_matrix)\n",
        "\n",
        "    summarized_text = self._get_summarized_text(sentence_tokenized, scores, max_largest_index)\n",
        "    # if index == 84:\n",
        "    #   print(summarized_text)\n",
        "\n",
        "    return summarized_text\n",
        "    \n"
      ],
      "metadata": {
        "id": "MI56GV0WY5gM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "tourism_news_ai.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMssNVSxa6lYuCXhCc94+RE"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}